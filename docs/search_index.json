[["index.html", "Data Science Toolbook Contents ", " Data Science Toolbook Heather Song Contents "],["variable-types-and-symbols.html", "Variable Types and Symbols", " Variable Types and Symbols Since \\(X_j\\) is \\(n\\times p\\) column vector, although we called \\(X\\) as a matrix of \\(n\\times p\\), actually, \\(X\\) is \\(X^T\\). \\[ \\begin{align} input \\ vector: X_j &amp; = \\begin{bmatrix} x_{j,1}\\\\x_{j,2}\\\\ ..\\\\ x_{j,p} \\\\ \\end{bmatrix}\\\\ input \\ matrix: X_{n\\times p} &amp;= \\begin{bmatrix} x_{1,1},x_{1,2},......x_{1,p} \\\\ x_{2,1},x_{2,2},......x_{2,p} \\\\ .................. \\\\ x_{n,1},x_{n,2},......x_{n,p} \\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} X_1^T\\\\ X_2^T\\\\ .\\\\ .\\\\ X_n^T \\end{bmatrix}\\\\ estimation &amp;=\\hat Y \\end{align} \\] "],["machine-learning.html", "Chapter 1 Machine Learning", " Chapter 1 Machine Learning This is a chapter of machine learning. Reference of book ESLII. "],["least-square.html", "1.1 Least Square", " 1.1 Least Square Least Square is most simple method for quantative prediction, which makes huge assumptions about structure and yields stable but possibly inaccurate predictions. 1.1.1 Basic Functions Basic functions are as follows: \\(X\\) is a \\(1\\times n\\) column vector and \\(\\hat Y\\) is a single output: \\[ \\begin{align} Input: vector \\ X^T &amp;= \\begin{bmatrix} X_1, X_2,......, X_p \\end{bmatrix} \\\\ Coefficients: \\ vector \\ \\beta&amp;= \\begin{bmatrix} \\hat\\beta_0\\\\ \\hat\\beta_1\\\\ ... \\\\ \\hat\\beta_p \\end{bmatrix} \\\\ Prediction: scalar\\ \\hat Y &amp;= \\hat \\beta_0+\\sum_{j=1}^p X_j \\hat \\beta_j \\\\ &amp;=X^T\\beta \\end{align} \\] \\(X\\) is a \\(n\\times p\\) column vector and \\(\\hat Y\\) is a \\(n\\times 1\\) column vector: \\[ \\begin{align} Input \\ matrix: X_{n\\times p} &amp;= \\begin{bmatrix} X_1, X_2,......, X_p \\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} ...... ,x_{1j} ,......\\\\ ......, x_{2j}, ......\\\\ ......,x_{3j} ,......\\\\ .................\\\\ ......,x_{nj}, ......\\end{bmatrix}\\\\ Prediction: \\hat Y_{n\\times 1} &amp;= \\hat \\beta_0+\\sum_{j=1}^p X_j \\hat \\beta_j\\\\ &amp;= \\hat \\beta_0 + \\begin{bmatrix} x_{11}\\\\ x_{21}\\\\ x_{31}\\\\ ..\\\\ x_{n1}\\end{bmatrix}\\hat \\beta_1+... + \\begin{bmatrix} x_{1j}\\\\ x_{2j}\\\\ x_{3j}\\\\ ..\\\\ x_{nj}\\end{bmatrix}\\hat \\beta_j+... + \\begin{bmatrix} x_{1p}\\\\ x_{2p}\\\\ x_{3p}\\\\ ..\\\\ x_{np}\\end{bmatrix}\\hat \\beta_p\\\\ &amp;=X\\beta \\\\ Bias \\ \\hat \\beta_0 &amp;: n\\times 1 \\\\ X_j &amp;: n\\times 1 \\\\ X &amp;: n\\times p \\\\ \\beta&amp;: p\\times 1 \\end{align} \\] \\(X\\) is a \\(n\\times p\\) column vector and \\(\\hat Y\\) is a \\(n\\times K\\) matrix (fit different combinations of coefficients for different prediction): \\[ \\begin{align} Input \\ matrix: X_{n\\times p} &amp;= \\begin{bmatrix} X_1, X_2,......, X_p \\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} ...... ,x_{1j} ,......\\\\ ......, x_{2j}, ......\\\\ ......,x_{3j} ,......\\\\ .................\\\\ ......,x_{nj}, ......\\end{bmatrix}\\\\ Prediction: \\hat Y_{n\\times K} &amp;= \\hat \\beta_0+\\sum_{j=1}^p X_j \\hat \\beta_j\\\\ &amp;= \\begin{bmatrix} \\hat\\beta_{01},\\hat\\beta_{02},...,\\hat\\beta_{0K}\\\\ \\hat\\beta_{01},\\hat\\beta_{02},...,\\hat\\beta_{0K}\\\\ \\hat\\beta_{01},\\hat\\beta_{02},...,\\hat\\beta_{0K} \\end{bmatrix} + \\begin{bmatrix} x_{11}\\\\ x_{21}\\\\ x_{31}\\\\ ..\\\\ x_{n1}\\end{bmatrix}\\begin{bmatrix} \\hat\\beta_{11},\\hat\\beta_{12},...,\\hat\\beta_{1K} \\end{bmatrix} +... + \\begin{bmatrix} x_{1j}\\\\ x_{2j}\\\\ x_{3j}\\\\ ..\\\\ x_{nj}\\end{bmatrix} \\begin{bmatrix} \\hat\\beta_{j1},\\hat\\beta_{j2},...,\\hat\\beta_{jK} \\end{bmatrix} +... + \\begin{bmatrix} x_{1p}\\\\ x_{2p}\\\\ x_{3p}\\\\ ..\\\\ x_{np}\\end{bmatrix}\\begin{bmatrix} \\hat\\beta_{p1},\\hat\\beta_{p2},...,\\hat\\beta_{pK} \\end{bmatrix} \\\\ Bias \\ \\hat \\beta_0 : n\\times K \\\\ X_j : n\\times 1 \\end{align} \\] Summarize as: \\[ \\begin{align} f(X) &amp;=\\hat Y= X \\boldsymbol{\\hat \\beta} \\\\ \\hat Y &amp;: n\\times 1 \\\\ X &amp;: n\\times (p+1) \\\\ \\boldsymbol{\\hat \\beta} &amp;: (p+1)\\times 1 \\end{align} \\] 1.1.2 Objection Minimizing Residual Sum Square (RSS) is usually used to fit coefficients. Since RSS is a quandratic function of \\(\\beta\\), therefore its minimum always exists. \\[ \\begin{align} RSS(\\beta) &amp;=\\sum_{i=1}^N (y_i-X_i^T\\beta)^2 \\\\ &amp;= (Y-X\\beta)^T(Y-X\\beta) \\end{align} \\] DIfferenciate RSS: \\[ \\begin{align} \\frac{dRSS}{d\\beta}&amp;=X^T(Y-X\\beta)=0\\\\ X^TX\\beta&amp;=X^TY \\\\ \\beta&amp;=(X^TX)^{-1}X^TY \\end{align} \\] "],["nearest-neighbor.html", "1.2 Nearest Neighbor", " 1.2 Nearest Neighbor 1.2.1 Basic Functions \\(N_k(x)\\) is neighbor of x, assuming Euclidean distance. \\[ \\begin{align} \\hat Y(x) &amp;=\\frac{1}{k}\\sum_{x_i\\in N_k(x)} y_i \\\\ \\end{align} \\] "],["searchingfor.html", "Chapter 2 SearchingFor", " Chapter 2 SearchingFor nonsingular mixture of gaussian "],["missing-values.html", "Chapter 3 Missing values", " Chapter 3 Missing values "],["results.html", "Chapter 4 Results", " Chapter 4 Results "],["interactive-component.html", "Chapter 5 Interactive component", " Chapter 5 Interactive component "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion "]]
