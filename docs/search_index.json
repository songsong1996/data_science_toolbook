[["index.html", "Data Science Toolbook Contents ", " Data Science Toolbook Heather Song Contents "],["variable-types-and-symbols.html", "Variable Types and Symbols", " Variable Types and Symbols Since \\(X_j\\) is \\(n\\times p\\) column vector, although we called \\(X\\) as a matrix of \\(n\\times p\\), actually, \\(X\\) is \\(X^T\\). \\[ \\begin{align} input \\ vector: X_j &amp; = \\begin{bmatrix} x_{j,1}\\\\x_{j,2}\\\\ ..\\\\ x_{j,p} \\\\ \\end{bmatrix}\\\\ input \\ matrix: X_{n\\times p} &amp;= \\begin{bmatrix} x_{1,1},x_{1,2},......x_{1,p} \\\\ x_{2,1},x_{2,2},......x_{2,p} \\\\ .................. \\\\ x_{n,1},x_{n,2},......x_{n,p} \\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} X_1^T\\\\ X_2^T\\\\ .\\\\ .\\\\ X_n^T \\end{bmatrix}\\\\ estimation &amp;=\\hat Y \\end{align} \\] "],["machine-learning.html", "Chapter 1 Machine Learning", " Chapter 1 Machine Learning This is a chapter of machine learning. Reference of book ESLII. "],["least-square.html", "1.1 Least Square", " 1.1 Least Square Least Square is most simple method for quantative prediction, which makes huge assumptions about structure and yields stable but possibly inaccurate predictions. 1.1.1 Basic Functions Basic functions are as follows: \\(X\\) is a \\(1\\times n\\) column vector and \\(\\hat Y\\) is a single output: \\[ \\begin{align} Input: vector \\ X^T &amp;= \\begin{bmatrix} X_1, X_2,......, X_p \\end{bmatrix} \\\\ Coefficients: \\ vector \\ \\beta&amp;= \\begin{bmatrix} \\hat\\beta_0\\\\ \\hat\\beta_1\\\\ ... \\\\ \\hat\\beta_p \\end{bmatrix} \\\\ Prediction: scalar\\ \\hat Y &amp;= \\hat \\beta_0+\\sum_{j=1}^p X_j \\hat \\beta_j \\\\ &amp;=X^T\\beta \\end{align} \\] \\(X\\) is a \\(n\\times p\\) column vector and \\(\\hat Y\\) is a \\(n\\times 1\\) column vector: \\[ \\begin{align} Input \\ matrix: X_{n\\times p} &amp;= \\begin{bmatrix} X_1, X_2,......, X_p \\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} ...... ,x_{1j} ,......\\\\ ......, x_{2j}, ......\\\\ ......,x_{3j} ,......\\\\ .................\\\\ ......,x_{nj}, ......\\end{bmatrix}\\\\ Prediction: \\hat Y_{n\\times 1} &amp;= \\hat \\beta_0+\\sum_{j=1}^p X_j \\hat \\beta_j\\\\ &amp;= \\hat \\beta_0 + \\begin{bmatrix} x_{11}\\\\ x_{21}\\\\ x_{31}\\\\ ..\\\\ x_{n1}\\end{bmatrix}\\hat \\beta_1+... + \\begin{bmatrix} x_{1j}\\\\ x_{2j}\\\\ x_{3j}\\\\ ..\\\\ x_{nj}\\end{bmatrix}\\hat \\beta_j+... + \\begin{bmatrix} x_{1p}\\\\ x_{2p}\\\\ x_{3p}\\\\ ..\\\\ x_{np}\\end{bmatrix}\\hat \\beta_p\\\\ &amp;=X\\beta \\\\ Bias \\ \\hat \\beta_0 &amp;: n\\times 1 \\\\ X_j &amp;: n\\times 1 \\\\ X &amp;: n\\times p \\\\ \\beta&amp;: p\\times 1 \\end{align} \\] \\(X\\) is a \\(n\\times p\\) column vector and \\(\\hat Y\\) is a \\(n\\times K\\) matrix (fit different combinations of coefficients for different prediction): \\[ \\begin{align} Input \\ matrix: X_{n\\times p} &amp;= \\begin{bmatrix} X_1, X_2,......, X_p \\end{bmatrix}\\\\ &amp;=\\begin{bmatrix} ...... ,x_{1j} ,......\\\\ ......, x_{2j}, ......\\\\ ......,x_{3j} ,......\\\\ .................\\\\ ......,x_{nj}, ......\\end{bmatrix}\\\\ Prediction: \\hat Y_{n\\times K} &amp;= \\hat \\beta_0+\\sum_{j=1}^p X_j \\hat \\beta_j\\\\ &amp;= \\begin{bmatrix} \\hat\\beta_{01},\\hat\\beta_{02},...,\\hat\\beta_{0K}\\\\ \\hat\\beta_{01},\\hat\\beta_{02},...,\\hat\\beta_{0K}\\\\ \\hat\\beta_{01},\\hat\\beta_{02},...,\\hat\\beta_{0K} \\end{bmatrix} + \\begin{bmatrix} x_{11}\\\\ x_{21}\\\\ x_{31}\\\\ ..\\\\ x_{n1}\\end{bmatrix}\\begin{bmatrix} \\hat\\beta_{11},\\hat\\beta_{12},...,\\hat\\beta_{1K} \\end{bmatrix} +... + \\begin{bmatrix} x_{1j}\\\\ x_{2j}\\\\ x_{3j}\\\\ ..\\\\ x_{nj}\\end{bmatrix} \\begin{bmatrix} \\hat\\beta_{j1},\\hat\\beta_{j2},...,\\hat\\beta_{jK} \\end{bmatrix} +... + \\begin{bmatrix} x_{1p}\\\\ x_{2p}\\\\ x_{3p}\\\\ ..\\\\ x_{np}\\end{bmatrix}\\begin{bmatrix} \\hat\\beta_{p1},\\hat\\beta_{p2},...,\\hat\\beta_{pK} \\end{bmatrix} \\\\ Bias \\ \\hat \\beta_0 : n\\times K \\\\ X_j : n\\times 1 \\end{align} \\] Summarize as: \\[ \\begin{align} f(X) &amp;=\\hat Y= X \\boldsymbol{\\hat \\beta} \\\\ \\hat Y &amp;: n\\times 1 \\\\ X &amp;: n\\times (p+1) \\\\ \\boldsymbol{\\hat \\beta} &amp;: (p+1)\\times 1 \\end{align} \\] 1.1.2 Objection Minimizing Residual Sum Square (RSS) is usually used to fit coefficients. Since RSS is a quandratic function of \\(\\beta\\), therefore its minimum always exists. \\[ \\begin{align} RSS(\\beta) &amp;=\\sum_{i=1}^N (y_i-X_i^T\\beta)^2 \\\\ &amp;= (Y-X\\beta)^T(Y-X\\beta) \\end{align} \\] DIfferenciate RSS: \\[ \\begin{align} \\frac{dRSS}{d\\beta}&amp;=X^T(Y-X\\beta)=0\\\\ X^TX\\beta&amp;=X^TY \\\\ \\beta&amp;=(X^TX)^{-1}X^TY \\end{align} \\] 1.1.3 Summary More fit for Scenario: The training data in each class were generated from bivariate Gaussian distributions with uncorrelated components and different means. "],["nearest-neighbor.html", "1.2 Nearest Neighbor", " 1.2 Nearest Neighbor 1.2.1 Basic Functions \\(N_k(x)\\) is neighbor of x, assuming Euclidean distance. \\[ \\begin{align} \\hat Y(x) &amp;=\\frac{1}{k}\\sum_{x_i\\in N_k(x)} y_i \\\\ \\end{align} \\] 1.2.2 Summary Effective number of parameters for KNN is \\(N/k\\) while for MLS is p (number of \\(\\beta_i\\)), although it seems the number of neighbors k is number of parameter of KNN while p is number of parameter of MLS. And \\(N/k\\) is usually larger than p. To get an idea of why, if all neighborhoods are nonoverlapping, KNN computes \\(N/k\\) parameters (one mean for each neighborhoods). More fit for Scenario: The training data in each class came from a mixture of 10 low- variance Gaussian distributions, with individual means themselves distributed as Gaussian. We cannot use Sum Square Error for KNN, because it will always goes to k=1. 1.2.3 From Least Square to Nearest Neighbor Least Square K Nearest Neighbor Assumption a linear boundary is appropriate no straight assumption Reason Depends on hadful points and their particular position, slightly changes may cause result changing. Variance low high Bias high low Stability unstable Scenario The training data in each class were generated from bivariate Gaussian distributions with uncorrelated components and different means. The training data in each class came from a mixture of 10 low- variance Gaussian distributions, with individual means themselves distributed as Gaussian. Methods to enhance KNN and LS - Kernel methods use weights that decrease smoothly to zero with dis- tance from the target point, rather than the effective 0/1 weights used by k-nearest neighbors. - In high-dimensional spaces the distance kernels are modified to em-phasize some variable more than others. - Local regression fits linear models by locally weighted least squares, rather than fitting constants locally. - Linear models fit to a basis expansion of the original inputs allow arbitrarily complex models. - Projection pursuit and neural network models consist of sums of non-linearly transformed linear models. "],["searchingfor.html", "Chapter 2 SearchingFor", " Chapter 2 SearchingFor nonsingular mixture of gaussian "],["missing-values.html", "Chapter 3 Missing values", " Chapter 3 Missing values "],["results.html", "Chapter 4 Results", " Chapter 4 Results "],["interactive-component.html", "Chapter 5 Interactive component", " Chapter 5 Interactive component "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion "]]
